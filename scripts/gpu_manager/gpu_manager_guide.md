# GPU ç®¡ç†å™¨å®Œæ•´ä½¿ç”¨æŒ‡å—

## ğŸ“‹ ç›®å½•

- [æ¦‚è¿°](#æ¦‚è¿°)
- [æ ¸å¿ƒæ¶æ„](#æ ¸å¿ƒæ¶æ„)
- [GPU è·å–ç­–ç•¥è¯¦è§£](#gpuè·å–ç­–ç•¥è¯¦è§£)
- [é…ç½®å‚æ•°è¯´æ˜](#é…ç½®å‚æ•°è¯´æ˜)
- [è¿è¡Œæ¨¡å¼è¯¦è§£](#è¿è¡Œæ¨¡å¼è¯¦è§£)
- [æ™ºèƒ½å†³ç­–ç®—æ³•](#æ™ºèƒ½å†³ç­–ç®—æ³•)
- [ä½¿ç”¨æŒ‡å—](#ä½¿ç”¨æŒ‡å—)
- [æ•…éšœæ’é™¤](#æ•…éšœæ’é™¤)
- [æ€§èƒ½ä¼˜åŒ–](#æ€§èƒ½ä¼˜åŒ–)

---

## ğŸ¯ æ¦‚è¿°

ç»¼åˆ GPU èµ„æºç®¡ç†å™¨æ˜¯ä¸€ä¸ªä¸º Notre Dame CRC ç¯å¢ƒè®¾è®¡çš„æ™ºèƒ½åŒ– GPU èµ„æºç®¡ç†å·¥å…·ã€‚å®ƒé›†æˆäº† GPU èµ„æºç›‘æ§ã€è‡ªåŠ¨ç”³è¯·é‡Šæ”¾ã€ç³»ç»Ÿèµ„æºç›‘æ§å’Œä½œä¸šçŠ¶æ€è·Ÿè¸ªç­‰åŠŸèƒ½ï¼Œæä¾›ä¸€ç«™å¼çš„ GPU è®¡ç®—èµ„æºç®¡ç†è§£å†³æ–¹æ¡ˆã€‚

### ä¸»è¦ç‰¹æ€§

- **ğŸ¤– æ™ºèƒ½è‡ªåŠ¨ç®¡ç†**: åŸºäºé…ç½®ç­–ç•¥è‡ªåŠ¨ç”³è¯·å’Œç»´æŠ¤ GPU èµ„æº
- **ğŸ“Š å®æ—¶ç›‘æ§**: å®æ—¶æ˜¾ç¤ºé›†ç¾¤çŠ¶æ€ã€ä½œä¸šä¿¡æ¯å’Œç³»ç»Ÿèµ„æº
- **ğŸ›ï¸ çµæ´»é…ç½®**: æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼å’Œç­–ç•¥å‚æ•°
- **ğŸ›¡ï¸ å®¹é”™æœºåˆ¶**: å®Œå–„çš„é”™è¯¯å¤„ç†å’Œæ¢å¤æœºåˆ¶
- **ğŸ“ˆ è¯¦ç»†ä¿¡æ¯**: æä¾›ä½œä¸šé˜Ÿåˆ—ä½ç½®ã€èµ„æºè¯·æ±‚ç­‰è¯¦ç»†ä¿¡æ¯

---

## ğŸ—ï¸ æ ¸å¿ƒæ¶æ„

### 3 å±‚æ¶æ„è®¾è®¡

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  ComprehensiveGPUManager  â”‚  â† ä¸»æ§åˆ¶å™¨
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  GPUMonitor         â”‚  â† ç›‘æ§å±‚
â”‚  SystemMonitor      â”‚  â† ç³»ç»Ÿç›‘æ§å±‚
â”‚  GPUJobManager      â”‚  â† ç®¡ç†å±‚
â”‚  StatusDisplay      â”‚  â† æ˜¾ç¤ºå±‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

#### **ç›‘æ§å±‚ (GPUMonitor)**

- **èŒè´£**: æ”¶é›† GPU èŠ‚ç‚¹çŠ¶æ€å’Œä½œä¸šä¿¡æ¯
- **æ•°æ®æº**: `free_gpus.sh`ã€`qstat -u user -xml`
- **åŠŸèƒ½**:
  - è·å–é›†ç¾¤ GPU èŠ‚ç‚¹çŠ¶æ€
  - è§£æç”¨æˆ· GPU ä½œä¸šè¯¦æƒ…
  - è®¡ç®—é˜Ÿåˆ—ä½ç½®å’Œèµ„æºè¯·æ±‚

#### **ç®¡ç†å±‚ (GPUJobManager)**

- **èŒè´£**: æ‰§è¡Œ GPU ç”³è¯·å’Œé‡Šæ”¾æ“ä½œ
- **æ“ä½œ**: `qsub`ã€`qdel`
- **åŠŸèƒ½**:
  - å•ä¸ª GPU ç”³è¯·
  - æ‰¹é‡ GPU ç”³è¯·
  - ä½œä¸šé‡Šæ”¾ç®¡ç†

#### **ç³»ç»Ÿç›‘æ§å±‚ (SystemMonitor)**

- **èŒè´£**: æ”¶é›†ç³»ç»Ÿèµ„æºçŠ¶æ€
- **æ•°æ®æº**: `/proc/stat`ã€`free`ã€`df`ã€`/proc/loadavg`
- **åŠŸèƒ½**:
  - CPU ä½¿ç”¨ç‡ç›‘æ§
  - å†…å­˜ä½¿ç”¨ç‡ç›‘æ§
  - ç£ç›˜ç©ºé—´ç›‘æ§
  - ç³»ç»Ÿè´Ÿè½½ç›‘æ§

#### **æ˜¾ç¤ºå±‚ (ComprehensiveStatusDisplay)**

- **èŒè´£**: æ ¼å¼åŒ–å’Œå±•ç¤ºçŠ¶æ€ä¿¡æ¯
- **åŠŸèƒ½**:
  - æ¨ªå‘å¸ƒå±€ä¼˜åŒ–
  - å®æ—¶çŠ¶æ€æ›´æ–°
  - è¯¦ç»†ä½œä¸šä¿¡æ¯å±•ç¤º

---

## ğŸ® GPU è·å–ç­–ç•¥è¯¦è§£

### è‡ªåŠ¨ç®¡ç†å†³ç­–æµç¨‹

```mermaid
graph TD
    A[å¯åŠ¨è‡ªåŠ¨ç®¡ç†] --> B[æ¯2ç§’æ£€æŸ¥]
    B --> C[è·å–é›†ç¾¤çŠ¶æ€]
    C --> D[è·å–æˆ‘çš„ä½œä¸š]
    D --> E{auto_manageå¼€å¯?}
    E -->|å¦| F[è·³è¿‡ç®¡ç†]
    E -->|æ˜¯| G[è®¡ç®—å½“å‰ç®¡ç†GPUæ•°é‡]
    G --> H[ç»Ÿè®¡è¿è¡Œä¸­+æ’é˜Ÿä¸­GPU]
    H --> I{æ€»æ•° < æœ€å°è¦æ±‚?}
    I -->|å¦| J[ç»´æŒç°çŠ¶]
    I -->|æ˜¯| K{æ€»æ•° < æœ€å¤§é™åˆ¶?}
    K -->|å¦| L[è¾¾åˆ°ä¸Šé™]
    K -->|æ˜¯| M{é›†ç¾¤æœ‰ç©ºé—²GPU?}
    M -->|å¦| N[ç­‰å¾…èµ„æº]
    M -->|æ˜¯| O[è®¡ç®—ç”³è¯·æ•°é‡]
    O --> P[æ‰§è¡ŒGPUç”³è¯·]
    P --> Q[ç”Ÿæˆä½œä¸šåç§°]
    Q --> R[æäº¤SGEä½œä¸š]
    R --> S{ç”³è¯·æˆåŠŸ?}
    S -->|æ˜¯| T[ç»§ç»­ç”³è¯·å‰©ä½™]
    S -->|å¦| U[åœæ­¢ç”³è¯·]
```

### ä½œä¸šè¯†åˆ«ç­–ç•¥

ç¨‹åºé€šè¿‡ä»¥ä¸‹æ–¹å¼è¯†åˆ«å’Œç®¡ç† GPU ä½œä¸šï¼š

#### **1. ä½œä¸šåç§°å‰ç¼€è¯†åˆ«**

```python
job_prefix = "NDFS_"  # é»˜è®¤å‰ç¼€
managed_jobs = [job for job in jobs if job.task_name.startswith(job_prefix)]
```

#### **2. GPU ä½œä¸šæ£€æµ‹**

```python
# æ–¹æ³•1: è¿è¡Œä¸­ä½œä¸šé€šè¿‡é˜Ÿåˆ—åæ£€æµ‹
if queue_name and "gpu@" in queue_name:
    is_gpu_job = True

# æ–¹æ³•2: æ’é˜Ÿä½œä¸šé€šè¿‡èµ„æºè¯·æ±‚æ£€æµ‹
if "gpu_card" in hard_resource_list:
    is_gpu_job = True
```

#### **3. èµ„æºæ•°é‡è§£æ**

```python
# GPUæ•°é‡: ä»resource_requestsè§£æ
resource_requests = self._get_job_resources(job_id)
gpu_count = int(resource_requests["gpu_card"]) if "gpu_card" in resource_requests else 1

# CPUæ•°é‡: ä»slotså­—æ®µè·å–
cpu_count = int(slots_text) if slots_text.isdigit() else 1
```

### ç”³è¯·æ•°é‡è®¡ç®—ç®—æ³•

```python
def calculate_needed_gpus(current_managed, min_required, max_total):
    """
    è®¡ç®—éœ€è¦ç”³è¯·çš„GPUæ•°é‡

    Args:
        current_managed: å½“å‰ç®¡ç†çš„GPUæ€»æ•° (è¿è¡Œä¸­+æ’é˜Ÿä¸­)
        min_required: æœ€å°ä¿ç•™GPUæ•°é‡
        max_total: æœ€å¤§GPUæ€»æ•°é™åˆ¶

    Returns:
        needed: éœ€è¦ç”³è¯·çš„GPUæ•°é‡
    """
    if current_managed >= min_required:
        return 0  # å·²æ»¡è¶³æœ€å°è¦æ±‚

    gap_to_min = min_required - current_managed
    gap_to_max = max_total - current_managed

    needed = min(gap_to_min, gap_to_max)
    return max(0, needed)
```

---

## âš™ï¸ é…ç½®å‚æ•°è¯´æ˜

### é»˜è®¤é…ç½® (`gpu_manager_config_simplified.json`)

```json
{
  "monitor_interval": 2, // ç›‘æ§é—´éš”æ—¶é—´(ç§’)
  "min_reserved_gpus": 1, // æœ€å°ä¿ç•™GPUæ•°é‡
  "max_reserved_gpus": 2, // æœ€å¤§ä¿ç•™GPUæ•°é‡(å½“å‰æœªä½¿ç”¨)
  "max_total_gpus": 4, // æœ€å¤§æ€»GPUæ•°é‡é™åˆ¶
  "job_prefix": "NDFS_", // ç®¡ç†ä½œä¸šåç§°å‰ç¼€
  "auto_manage": true // è‡ªåŠ¨ç®¡ç†å¼€å…³
}
```

### å‚æ•°è¯¦ç»†è¯´æ˜

| å‚æ•°                | ç±»å‹ | é»˜è®¤å€¼   | è¯´æ˜                                      |
| ------------------- | ---- | -------- | ----------------------------------------- |
| `monitor_interval`  | int  | 2        | çŠ¶æ€æ£€æŸ¥é—´éš”(ç§’)ï¼Œå»ºè®® 1-5 ç§’             |
| `min_reserved_gpus` | int  | 1        | æœ€å°‘ä¿æŒçš„ GPU æ•°é‡ï¼Œä½äºæ­¤å€¼ä¼šè‡ªåŠ¨ç”³è¯·   |
| `max_reserved_gpus` | int  | 2        | æœ€å¤§ä¿ç•™æ•°é‡(æœªå®ç°)ï¼Œé¢„ç•™ç»™æœªæ¥æ‰©å±•      |
| `max_total_gpus`    | int  | 4        | ç»å¯¹æœ€å¤§ GPU æ•°é‡ï¼Œé˜²æ­¢è¿‡åº¦ç”³è¯·           |
| `job_prefix`        | str  | "NDFS\_" | ç®¡ç†ä½œä¸šçš„å‰ç¼€æ ‡è¯†ï¼Œç”¨äºåŒºåˆ†è‡ªåŠ¨/æ‰‹åŠ¨ä½œä¸š |
| `auto_manage`       | bool | true     | è‡ªåŠ¨ç®¡ç†å¼€å…³ï¼Œfalse æ—¶ä»…ç›‘æ§ä¸ç”³è¯·        |

### é…ç½®ä¼˜åŒ–å»ºè®®

#### **è½»é‡çº§é…ç½®** (å•ç”¨æˆ·ï¼Œå¶å°”ä½¿ç”¨)

```json
{
  "monitor_interval": 5,
  "min_reserved_gpus": 1,
  "max_total_gpus": 2,
  "auto_manage": true
}
```

#### **ç§¯æé…ç½®** (å›¢é˜Ÿä½¿ç”¨ï¼Œé¢‘ç¹è®¡ç®—)

```json
{
  "monitor_interval": 1,
  "min_reserved_gpus": 2,
  "max_total_gpus": 8,
  "auto_manage": true
}
```

#### **ä¿å®ˆé…ç½®** (èµ„æºç´§å¼ ç¯å¢ƒ)

```json
{
  "monitor_interval": 10,
  "min_reserved_gpus": 1,
  "max_total_gpus": 2,
  "auto_manage": false
}
```

---

## ğŸ›ï¸ è¿è¡Œæ¨¡å¼è¯¦è§£

### 1. ç›‘æ§æ¨¡å¼ (é»˜è®¤)

```bash
python scripts/gpu_manager.py
# æˆ–
python scripts/gpu_manager.py --monitor
```

**ç‰¹ç‚¹**:

- ä»…ç›‘æ§çŠ¶æ€ï¼Œä¸æ‰§è¡Œè‡ªåŠ¨ç”³è¯·
- å®æ—¶æ˜¾ç¤ºé›†ç¾¤å’Œä½œä¸šä¿¡æ¯
- é€‚åˆçŠ¶æ€è§‚å¯Ÿå’Œè°ƒè¯•

**è¾“å‡ºç¤ºä¾‹**:

```
================================================================================
ğŸ–¥ï¸  ç»¼åˆGPUèµ„æºç®¡ç†å™¨ - 2025-07-22 02:30:00
ğŸ‘¤ ç”¨æˆ·: zchen27
================================================================================

ğŸ“Š é›†ç¾¤çŠ¶æ€                        ğŸ’» ç³»ç»Ÿèµ„æº
   æ€»GPUæ•°: 44                     CPUä½¿ç”¨ç‡: 7.3%
   ç©ºé—²GPU: 0                      å†…å­˜ä½¿ç”¨ç‡: 65.8%
   GPUä½¿ç”¨ç‡: 100.0%           ç£ç›˜ä½¿ç”¨ç‡: 21.0%
   æˆ‘çš„è¿è¡ŒCPU: 25              ç³»ç»Ÿè´Ÿè½½: 3.92
   æˆ‘çš„æ’é˜ŸCPU: 0               è¿è¡Œæ—¶é—´: 44:24:47
                                           è¿›ç¨‹æ•°: 1452
```

### 2. è‡ªåŠ¨ç®¡ç†æ¨¡å¼

```bash
python scripts/gpu_manager.py --manage
```

**ç‰¹ç‚¹**:

- å¯ç”¨è‡ªåŠ¨ GPU ç”³è¯·å’Œé‡Šæ”¾
- æ ¹æ®é…ç½®ç­–ç•¥æ™ºèƒ½å†³ç­–
- é€‚åˆé•¿æœŸè¿è¡Œå’Œæ— äººå€¼å®ˆ

**å†³ç­–é€»è¾‘**:

```python
while running:
    current_gpus = count_managed_gpus()
    cluster_free = count_free_gpus()

    if current_gpus < min_required and cluster_free > 0:
        needed = min(min_required - current_gpus, max_total - current_gpus)
        for i in range(needed):
            request_gpu(f"auto_{i+1}")
```

### 3. æ‰‹åŠ¨æ“ä½œæ¨¡å¼

#### **ç”³è¯·æŒ‡å®šæ•°é‡ GPU**

```bash
python scripts/gpu_manager.py --request 3
```

#### **é‡Šæ”¾æŒ‡å®šä½œä¸š**

```bash
python scripts/gpu_manager.py --release 1911586
```

#### **å•æ¬¡çŠ¶æ€æŸ¥çœ‹**

```bash
python scripts/gpu_manager.py --status
```

### 4. Tmux ä¼šè¯ç®¡ç†

#### **åˆ›å»ºåå°ç®¡ç†ä¼šè¯**

```bash
# åˆ›å»ºè‡ªåŠ¨ç®¡ç†ä¼šè¯
tmux new-session -d -s gpu_manager 'python scripts/gpu_manager.py --manage'

# è¿›å…¥ä¼šè¯æŸ¥çœ‹
tmux attach -t gpu_manager

# åˆ†ç¦»ä¼šè¯ (Ctrl+B, D)
# ç»ˆæ­¢ä¼šè¯
tmux kill-session -t gpu_manager
```

---

## ğŸ§  æ™ºèƒ½å†³ç­–ç®—æ³•

### çŠ¶æ€è¯„ä¼°ç®—æ³•

```python
def evaluate_gpu_status(self, jobs: List[GPUJob]) -> dict:
    """è¯„ä¼°å½“å‰GPUçŠ¶æ€å¹¶ç”Ÿæˆå†³ç­–å»ºè®®"""

    # åˆ†ç±»ç»Ÿè®¡ç®¡ç†çš„ä½œä¸š
    managed_jobs = [job for job in jobs if job.task_name.startswith(self.job_prefix)]

    running_jobs = [job for job in managed_jobs if job.status == "running"]
    pending_jobs = [job for job in managed_jobs if job.status == "pending"]

    running_gpus = sum(job.gpu_count for job in running_jobs)
    pending_gpus = sum(job.gpu_count for job in pending_jobs)
    total_managed = running_gpus + pending_gpus

    # ç”ŸæˆçŠ¶æ€è¯„ä¼°
    status = {
        'total_managed': total_managed,
        'running_gpus': running_gpus,
        'pending_gpus': pending_gpus,
        'below_minimum': total_managed < self.config['min_reserved_gpus'],
        'at_maximum': total_managed >= self.config['max_total_gpus'],
        'recommendation': self._generate_recommendation(total_managed)
    }

    return status

def _generate_recommendation(self, current: int) -> str:
    """ç”Ÿæˆæ“ä½œå»ºè®®"""
    min_req = self.config['min_reserved_gpus']
    max_total = self.config['max_total_gpus']

    if current < min_req:
        needed = min(min_req - current, max_total - current)
        return f"å»ºè®®ç”³è¯· {needed} ä¸ªGPUä»¥æ»¡è¶³æœ€å°è¦æ±‚"
    elif current > max_total:
        excess = current - max_total
        return f"è¶…å‡ºé™åˆ¶ {excess} ä¸ªGPUï¼Œå»ºè®®é‡Šæ”¾éƒ¨åˆ†ä½œä¸š"
    else:
        return "GPUèµ„æºé…ç½®åˆç†ï¼Œç»´æŒç°çŠ¶"
```

### æ™ºèƒ½ç”³è¯·ç­–ç•¥

#### **æ—¶é—´çª—å£æ§åˆ¶**

```python
def should_request_now(self) -> bool:
    """åˆ¤æ–­æ˜¯å¦é€‚åˆç°åœ¨ç”³è¯·GPU"""

    # é¿å…åœ¨é›†ç¾¤ç»´æŠ¤æ—¶é—´ç”³è¯·
    current_hour = datetime.now().hour
    if 2 <= current_hour <= 6:  # å‡Œæ™¨2-6ç‚¹ç»´æŠ¤æ—¶é—´
        return False

    # æ£€æŸ¥æœ€è¿‘ç”³è¯·é¢‘ç‡
    if self.last_request_time:
        elapsed = time.time() - self.last_request_time
        if elapsed < 60:  # 1åˆ†é’Ÿå†…ä¸é‡å¤ç”³è¯·
            return False

    return True
```

#### **åŠ¨æ€ä¼˜å…ˆçº§è°ƒæ•´**

```python
def calculate_request_priority(self, current_load: float) -> int:
    """æ ¹æ®ç³»ç»Ÿè´Ÿè½½è®¡ç®—ç”³è¯·ä¼˜å…ˆçº§"""

    if current_load < 0.5:
        return 1  # é«˜ä¼˜å…ˆçº§ï¼Œç³»ç»Ÿç©ºé—²
    elif current_load < 1.0:
        return 2  # ä¸­ä¼˜å…ˆçº§ï¼Œé€‚åº¦è´Ÿè½½
    else:
        return 3  # ä½ä¼˜å…ˆçº§ï¼Œç³»ç»Ÿç¹å¿™
```

---

## ğŸ“š ä½¿ç”¨æŒ‡å—

### å¿«é€Ÿå¼€å§‹

#### **1. åŸºç¡€ç›‘æ§**

```bash
# è¿›å…¥é¡¹ç›®ç›®å½•
cd /users/zchen27/ND-Flexible-Sensor

# æŸ¥çœ‹å½“å‰çŠ¶æ€
python scripts/gpu_manager.py --status

# å¯åŠ¨å®æ—¶ç›‘æ§
python scripts/gpu_manager.py
```

#### **2. å¯ç”¨è‡ªåŠ¨ç®¡ç†**

```bash
# å‰å°è¿è¡Œè‡ªåŠ¨ç®¡ç†
python scripts/gpu_manager.py --manage

# åå°è¿è¡Œè‡ªåŠ¨ç®¡ç†
tmux new-session -d -s gpu_manager 'python scripts/gpu_manager.py --manage'
```

#### **3. æ‰‹åŠ¨æ“ä½œ**

```bash
# ç”³è¯·2ä¸ªGPU
python scripts/gpu_manager.py --request 2

# é‡Šæ”¾ä½œä¸š1911586
python scripts/gpu_manager.py --release 1911586

# æŸ¥çœ‹å¸®åŠ©
python scripts/gpu_manager.py --help
```

### é…ç½®å®šåˆ¶

#### **åˆ›å»ºè‡ªå®šä¹‰é…ç½®**

```bash
# å¤åˆ¶é»˜è®¤é…ç½®
cp gpu_manager_config_simplified.json my_config.json

# ç¼–è¾‘é…ç½®
vim my_config.json

# ä½¿ç”¨è‡ªå®šä¹‰é…ç½®
python scripts/gpu_manager.py --config my_config.json --manage
```

#### **é…ç½®ç¤ºä¾‹åœºæ™¯**

**ç§‘ç ”è®¡ç®—åœºæ™¯**:

```json
{
  "monitor_interval": 3,
  "min_reserved_gpus": 2,
  "max_total_gpus": 6,
  "job_prefix": "RESEARCH_",
  "auto_manage": true
}
```

**å¼€å‘æµ‹è¯•åœºæ™¯**:

```json
{
  "monitor_interval": 5,
  "min_reserved_gpus": 1,
  "max_total_gpus": 2,
  "job_prefix": "DEV_",
  "auto_manage": false
}
```

### é›†æˆåˆ°å·¥ä½œæµ

#### **ä¸ç ”ç©¶è„šæœ¬é›†æˆ**

```python
#!/usr/bin/env python3
"""ç ”ç©¶è„šæœ¬ç¤ºä¾‹"""

import subprocess
import time

def ensure_gpu_available():
    """ç¡®ä¿æœ‰GPUå¯ç”¨"""
    result = subprocess.run([
        "python", "scripts/gpu_manager.py", "--status"
    ], capture_output=True, text=True)

    if "è¿è¡Œä¸­GPU: 0" in result.stdout:
        print("ç”³è¯·GPUèµ„æº...")
        subprocess.run([
            "python", "scripts/gpu_manager.py", "--request", "1"
        ])
        time.sleep(30)  # ç­‰å¾…GPUå°±ç»ª

def main():
    ensure_gpu_available()
    # è¿è¡ŒGPUè®¡ç®—ä»»åŠ¡
    run_gpu_computation()

if __name__ == "__main__":
    main()
```

#### **æ‰¹å¤„ç†è„šæœ¬é›†æˆ**

```bash
#!/bin/bash
# batch_job.sh

# ç¡®ä¿GPUç®¡ç†å™¨è¿è¡Œ
tmux has-session -t gpu_manager 2>/dev/null || \
tmux new-session -d -s gpu_manager 'python scripts/gpu_manager.py --manage'

# ç­‰å¾…GPUå°±ç»ª
echo "ç­‰å¾…GPUèµ„æº..."
while [ $(python scripts/gpu_manager.py --status | grep "è¿è¡Œä¸­GPU" | cut -d: -f2 | cut -d' ' -f2) -eq 0 ]; do
    sleep 10
done

echo "GPUå°±ç»ªï¼Œå¼€å§‹è®¡ç®—ä»»åŠ¡..."
# æ‰§è¡Œå®é™…è®¡ç®—ä»»åŠ¡
python my_gpu_computation.py
```

---

## ğŸ”§ æ•…éšœæ’é™¤

### å¸¸è§é—®é¢˜åŠè§£å†³æ–¹æ¡ˆ

#### **1. GPU ç”³è¯·å¤±è´¥**

**ç—‡çŠ¶**: æ˜¾ç¤º"âŒ ç”³è¯·å¤±è´¥"
**å¯èƒ½åŸå› **:

- é›†ç¾¤èµ„æºä¸è¶³
- SGE é…ç½®é—®é¢˜
- ç½‘ç»œè¿æ¥é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥é›†ç¾¤çŠ¶æ€
free_gpus.sh @crc_gpu

# æ£€æŸ¥SGEé…ç½®
qconf -sql
qconf -sq gpu

# æ‰‹åŠ¨æµ‹è¯•ç”³è¯·
qsub -q gpu -l gpu_card=1 -b y sleep 60
```

#### **2. ä½œä¸šæ£€æµ‹ä¸åˆ°**

**ç—‡çŠ¶**: è¿è¡Œçš„ GPU ä½œä¸šä¸æ˜¾ç¤ºåœ¨çŠ¶æ€ä¸­
**å¯èƒ½åŸå› **:

- ä½œä¸šåç§°ä¸åŒ…å«ç®¡ç†å‰ç¼€
- qstat æƒé™é—®é¢˜
- XML è§£æé”™è¯¯

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥ç”¨æˆ·ä½œä¸š
qstat -u $USER

# æ£€æŸ¥XMLè¾“å‡º
qstat -u $USER -xml

# æ£€æŸ¥ä½œä¸šè¯¦æƒ…
qstat -j <job_id>
```

#### **3. é…ç½®æ–‡ä»¶é—®é¢˜**

**ç—‡çŠ¶**: ç¨‹åºä½¿ç”¨é»˜è®¤é…ç½®è€Œéè‡ªå®šä¹‰é…ç½®
**å¯èƒ½åŸå› **:

- é…ç½®æ–‡ä»¶è·¯å¾„é”™è¯¯
- JSON æ ¼å¼é”™è¯¯
- æƒé™é—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:

```bash
# éªŒè¯JSONæ ¼å¼
python -m json.tool gpu_manager_config_simplified.json

# æ£€æŸ¥æ–‡ä»¶æƒé™
ls -la gpu_manager_config_simplified.json

# ä½¿ç”¨ç»å¯¹è·¯å¾„
python scripts/gpu_manager.py --config /full/path/to/config.json
```

#### **4. Tmux ä¼šè¯é—®é¢˜**

**ç—‡çŠ¶**: æ— æ³•åˆ›å»ºæˆ–è¿æ¥ tmux ä¼šè¯
**å¯èƒ½åŸå› **:

- ä¼šè¯åå†²çª
- tmux ç‰ˆæœ¬é—®é¢˜
- ç»ˆç«¯ç¯å¢ƒé—®é¢˜

**è§£å†³æ–¹æ¡ˆ**:

```bash
# æ£€æŸ¥ç°æœ‰ä¼šè¯
tmux list-sessions

# ç»ˆæ­¢å†²çªä¼šè¯
tmux kill-session -t gpu_manager

# é‡æ–°åˆ›å»ºä¼šè¯
tmux new-session -d -s gpu_manager_new 'python scripts/gpu_manager.py --manage'
```

### è°ƒè¯•æŠ€å·§

#### **1. è¯¦ç»†æ—¥å¿—æ¨¡å¼**

```python
# åœ¨è„šæœ¬ä¸­æ·»åŠ è°ƒè¯•ä¿¡æ¯
import logging
logging.basicConfig(level=logging.DEBUG)

# æˆ–è®¾ç½®ç¯å¢ƒå˜é‡
export DEBUG=1
python scripts/gpu_manager.py --manage
```

#### **2. åˆ†æ­¥è°ƒè¯•**

```bash
# é€æ­¥æ£€æŸ¥å„ç»„ä»¶
python -c "
from scripts.gpu_manager import GPUMonitor
monitor = GPUMonitor()
print('Nodes:', monitor.get_gpu_nodes())
print('Jobs:', monitor.get_my_jobs())
"
```

#### **3. æ€§èƒ½åˆ†æ**

```bash
# ä½¿ç”¨timeå‘½ä»¤åˆ†ææ‰§è¡Œæ—¶é—´
time python scripts/gpu_manager.py --status

# ä½¿ç”¨straceè·Ÿè¸ªç³»ç»Ÿè°ƒç”¨
strace -c python scripts/gpu_manager.py --status
```

---

## ğŸš€ æ€§èƒ½ä¼˜åŒ–

### ç›‘æ§æ€§èƒ½ä¼˜åŒ–

#### **1. é™ä½æ£€æŸ¥é¢‘ç‡**

```json
{
  "monitor_interval": 5 // ä»2ç§’å¢åŠ åˆ°5ç§’
}
```

#### **2. ç¼“å­˜æœºåˆ¶**

```python
class CachedGPUMonitor(GPUMonitor):
    def __init__(self, cache_ttl=30):
        super().__init__()
        self.cache = {}
        self.cache_ttl = cache_ttl

    def get_gpu_nodes(self):
        now = time.time()
        if 'nodes' in self.cache:
            if now - self.cache['nodes']['timestamp'] < self.cache_ttl:
                return self.cache['nodes']['data']

        data = super().get_gpu_nodes()
        self.cache['nodes'] = {'data': data, 'timestamp': now}
        return data
```

#### **3. å¼‚æ­¥å¤„ç†**

```python
import asyncio
import concurrent.futures

class AsyncGPUManager:
    async def collect_all_status(self):
        with concurrent.futures.ThreadPoolExecutor() as executor:
            nodes_future = executor.submit(self.gpu_monitor.get_gpu_nodes)
            jobs_future = executor.submit(self.gpu_monitor.get_my_jobs)
            system_future = executor.submit(self.system_monitor.get_system_status)

            nodes = await asyncio.wrap_future(nodes_future)
            jobs = await asyncio.wrap_future(jobs_future)
            system_status = await asyncio.wrap_future(system_future)

            return nodes, jobs, system_status
```

### èµ„æºä½¿ç”¨ä¼˜åŒ–

#### **1. å†…å­˜ä¼˜åŒ–**

```python
# ä½¿ç”¨__slots__å‡å°‘å†…å­˜å ç”¨
@dataclass
class GPUJob:
    __slots__ = ['job_id', 'task_name', 'gpu_count', 'cpu_count', 'status', 'node']
```

#### **2. CPU ä¼˜åŒ–**

```python
# ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼ç¼–è¯‘
class GPUMonitor:
    def __init__(self):
        self.gpu_pattern = re.compile(r"Free GPU cards on (\S+) : (\d+)")
        self.job_pattern = re.compile(r"gpu_card=(\d+)")
```

#### **3. ç½‘ç»œä¼˜åŒ–**

```python
# å‡å°‘subprocessè°ƒç”¨
def batch_qstat_calls(job_ids):
    """æ‰¹é‡è·å–ä½œä¸šä¿¡æ¯è€Œéé€ä¸ªæŸ¥è¯¢"""
    if not job_ids:
        return {}

    cmd = ["qstat", "-j"] + job_ids
    result = subprocess.run(cmd, capture_output=True, text=True)
    return parse_batch_qstat_output(result.stdout)
```

### æ‰©å±•æ€§ä¼˜åŒ–

#### **1. æ’ä»¶æ¶æ„**

```python
class GPUManagerPlugin:
    def on_gpu_acquired(self, job_id: str):
        pass

    def on_gpu_released(self, job_id: str):
        pass

    def on_status_update(self, status: dict):
        pass

class NotificationPlugin(GPUManagerPlugin):
    def on_gpu_acquired(self, job_id: str):
        send_slack_notification(f"GPU acquired: {job_id}")
```

#### **2. é…ç½®çƒ­é‡è½½**

```python
import watchdog.events
import watchdog.observers

class ConfigWatcher(watchdog.events.FileSystemEventHandler):
    def __init__(self, manager):
        self.manager = manager

    def on_modified(self, event):
        if event.src_path.endswith('.json'):
            self.manager.reload_config()
```

#### **3. å¤šç”¨æˆ·æ”¯æŒ**

```python
class MultiUserGPUManager:
    def __init__(self):
        self.user_managers = {}

    def get_manager_for_user(self, username):
        if username not in self.user_managers:
            config_file = f"config_{username}.json"
            self.user_managers[username] = ComprehensiveGPUManager(config_file)
        return self.user_managers[username]
```

---

## ğŸ“Š æœ€ä½³å®è·µ

### 1. é…ç½®ç­–ç•¥

- **å¼€å‘é˜¶æ®µ**: `min_reserved_gpus: 1, max_total_gpus: 2, auto_manage: false`
- **å®éªŒé˜¶æ®µ**: `min_reserved_gpus: 2, max_total_gpus: 4, auto_manage: true`
- **ç”Ÿäº§é˜¶æ®µ**: `min_reserved_gpus: 1, max_total_gpus: 8, auto_manage: true`

### 2. ç›‘æ§ç­–ç•¥

- **å®æ—¶ä»»åŠ¡**: `monitor_interval: 1-2`ç§’
- **å¸¸è§„ä»»åŠ¡**: `monitor_interval: 3-5`ç§’
- **åå°ä»»åŠ¡**: `monitor_interval: 10-30`ç§’

### 3. èµ„æºç®¡ç†

- å®šæœŸæ¸…ç†è¿‡æœŸä½œä¸š
- ç›‘æ§èµ„æºä½¿ç”¨æ•ˆç‡
- æ ¹æ®éœ€æ±‚åŠ¨æ€è°ƒæ•´é…ç½®

### 4. æ•…éšœé¢„é˜²

- å®šæœŸå¤‡ä»½é…ç½®æ–‡ä»¶
- ç›‘æ§ç³»ç»Ÿæ—¥å¿—
- è®¾ç½®èµ„æºä½¿ç”¨å‘Šè­¦

---

## ğŸ“ æ›´æ–°æ—¥å¿—

### v2.0.0 (2025-07-22)

- âœ… ç»¼åˆ GPU ç®¡ç†å™¨é‡æ„å®Œæˆ
- âœ… ä¿®å¤ GPU/CPU æ•°é‡ç»Ÿè®¡é”™è¯¯
- âœ… ä¼˜åŒ–ç•Œé¢å¸ƒå±€ï¼Œæ¨ªå‘å¹¶æ’æ˜¾ç¤º
- âœ… ç§»é™¤ç½‘ç»œæ¥å£æ˜¾ç¤ºï¼Œä¸“æ³¨æ ¸å¿ƒåŠŸèƒ½
- âœ… å¢å¼ºä½œä¸šä¿¡æ¯æ˜¾ç¤ºï¼ŒåŒ…å«è¯¦ç»†èµ„æºä¿¡æ¯
- âœ… å®Œå–„é”™è¯¯å¤„ç†å’Œå®¹é”™æœºåˆ¶

### v1.0.0 (2025-07-21)

- âœ… åˆç‰ˆ GPU èµ„æºç®¡ç†å™¨å®Œæˆ
- âœ… å®ç°åŸºç¡€ç›‘æ§å’Œè‡ªåŠ¨ç®¡ç†åŠŸèƒ½
- âœ… ä¸­æ–‡åŒ–ç•Œé¢æ˜¾ç¤º
- âœ… æ”¯æŒå¤šç§è¿è¡Œæ¨¡å¼

---

## ğŸ”— ç›¸å…³æ–‡æ¡£

- [development_record.md](development_record.md) - å¼€å‘è®°å½•å’Œç‰ˆæœ¬å†å²
- [objective.md](objective.md) - é¡¹ç›®ç›®æ ‡å’Œè¿›å±•
- [gpu_manager_config_simplified.json](gpu_manager_config_simplified.json) - é»˜è®¤é…ç½®æ–‡ä»¶

---

## ğŸ“ æ”¯æŒä¸åé¦ˆ

å¦‚æœ‰é—®é¢˜æˆ–å»ºè®®ï¼Œè¯·è”ç³»å¼€å‘å›¢é˜Ÿæˆ–åœ¨é¡¹ç›®ä»“åº“æäº¤ Issueã€‚

**æœ€åæ›´æ–°**: 2025-07-22  
**æ–‡æ¡£ç‰ˆæœ¬**: v2.0.0
