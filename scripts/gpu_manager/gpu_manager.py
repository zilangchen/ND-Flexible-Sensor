#!/usr/bin/env python3
"""
Comprehensive GPU Resource Manager for Notre Dame CRC

A unified management system integrating GPU resource management 
with comprehensive system monitoring capabilities.
"""

import json
import os
import re
import subprocess
import threading
import time
import xml.etree.ElementTree as ET
from dataclasses import dataclass
from datetime import datetime
from typing import Dict, List, Optional
import getpass
import argparse
import signal
import sys


@dataclass
class GPUNode:
    """Simple GPU node representation"""
    name: str
    total_gpus: int
    free_gpus: int


@dataclass
class GPUJob:
    """Enhanced GPU job representation with detailed information"""
    job_id: str
    task_name: str
    gpu_count: int
    cpu_count: int  # æ–°å¢CPUæ•°é‡
    status: str  # 'running', 'pending'
    node: str
    submit_time: str = ""
    priority: str = ""
    queue_position: int = 0
    resource_requests: Dict[str, str] = None

    def __post_init__(self):
        if self.resource_requests is None:
            self.resource_requests = {}

    def get_state_description(self) -> str:
        """Get human readable state description"""
        state_map = {
            "r": "è¿è¡Œä¸­",
            "qw": "æ’é˜Ÿä¸­",
            "t": "ä¼ è¾“ä¸­",
            "s": "æš‚åœ",
            "h": "ä¿æŒ",
            "E": "é”™è¯¯"
        }
        return state_map.get(self.status, f"æœªçŸ¥({self.status})")

    def format_resources(self) -> str:
        """Format resource requests for display"""
        parts = []

        # æ·»åŠ GPUä¿¡æ¯
        if self.gpu_count > 0:
            parts.append(f"{self.gpu_count}GPU")

        # æ·»åŠ CPUä¿¡æ¯
        if self.cpu_count > 0:
            parts.append(f"{self.cpu_count}CPU")

        # æ·»åŠ å…¶ä»–èµ„æºä¿¡æ¯
        if self.resource_requests:
            for resource, value in self.resource_requests.items():
                if resource == "mem_free":
                    if value.endswith("G"):
                        parts.append(f"{value}-RAM")
                    else:
                        parts.append(f"{value}")
                elif resource == "h_vmem":
                    parts.append(f"{value}")
                elif resource not in ["gpu_card"]:  # è·³è¿‡å·²å¤„ç†çš„gpu_card
                    parts.append(f"{value}")

        return ",".join(parts)[:20] if parts else f"{self.gpu_count}GPU,{self.cpu_count}CPU"


@dataclass
class SystemStatus:
    """System resource status"""
    cpu_percent: float
    memory_percent: float
    disk_percent: float
    load_average: List[float]
    uptime: float
    processes: int

    def format_uptime(self) -> str:
        """Format uptime as HH:MM:SS"""
        total_seconds = int(self.uptime)
        minutes, seconds = divmod(total_seconds, 60)
        hours, minutes = divmod(minutes, 60)
        return f"{hours:02d}:{minutes:02d}:{seconds:02d}"


class GPUMonitor:
    """åŸºç¡€ç›‘æ§å±‚ - è´Ÿè´£æ”¶é›†GPUçŠ¶æ€"""

    def __init__(self):
        self.cmd = ["free_gpus.sh", "@crc_gpu"]
        self.pattern = re.compile(r"Free GPU cards on (\S+) : (\d+)")

    def get_gpu_nodes(self) -> Dict[str, GPUNode]:
        """è·å–æ‰€æœ‰GPUèŠ‚ç‚¹çŠ¶æ€"""
        try:
            result = subprocess.run(
                self.cmd, capture_output=True, text=True, check=True
            )
            nodes = {}
            for line in result.stdout.splitlines():
                match = self.pattern.match(line)
                if match:
                    node_name = match.group(1)
                    free_gpus = int(match.group(2))
                    nodes[node_name] = GPUNode(
                        name=node_name,
                        total_gpus=4,  # A10 nodes have 4 GPUs
                        free_gpus=free_gpus
                    )
            return nodes
        except (subprocess.CalledProcessError, FileNotFoundError):
            return {}

    def get_my_jobs(self) -> List[GPUJob]:
        """è·å–å½“å‰ç”¨æˆ·çš„GPUä½œä¸šï¼ŒåŒ…å«è¯¦ç»†ä¿¡æ¯"""
        try:
            user = getpass.getuser()
            result = subprocess.run(
                ["qstat", "-u", user, "-xml"],
                capture_output=True, text=True, check=True
            )

            jobs = []
            root = ET.fromstring(result.stdout)

            # Get queue positions for pending jobs
            queue_positions = self._get_queue_positions()

            for job_elem in root.findall(".//job_list"):
                job_id = job_elem.findtext("JB_job_number", "")
                task_name = job_elem.findtext("JB_name", "")
                state = job_elem.findtext("state", "")
                queue_name = job_elem.findtext("queue_name", "pending")
                slots_text = job_elem.findtext("slots", "1")
                submit_time = job_elem.findtext("JB_submission_time", "")
                priority = job_elem.findtext("JAT_prio", "")

                # Check if it's a GPU job by queue name or resource request
                is_gpu_job = False
                gpu_count = 0
                cpu_count = int(slots_text) if slots_text.isdigit() else 1

                # Method 1: For running jobs, check queue name
                if queue_name and "gpu@" in queue_name:
                    is_gpu_job = True
                    # For running GPU jobs, get GPU count from resource requests
                    resource_requests = self._get_job_resources(job_id)
                    if "gpu_card" in resource_requests:
                        gpu_count = int(resource_requests["gpu_card"])
                    else:
                        # é»˜è®¤å‡è®¾GPUä½œä¸šè‡³å°‘æœ‰1ä¸ªGPU
                        gpu_count = 1

                # Method 2: For pending jobs, check hard_resource_list
                if not is_gpu_job:
                    hard_resource = job_elem.find(".//hard_resource_list")
                    if hard_resource is not None:
                        resource_text = hard_resource.text or ""
                        if "gpu_card" in resource_text:
                            is_gpu_job = True
                            # Extract GPU count from resource request
                            gpu_match = re.search(
                                r"gpu_card=(\d+)", resource_text)
                            gpu_count = int(gpu_match.group(1)
                                            ) if gpu_match else 1

                # Only include GPU jobs
                if not is_gpu_job:
                    continue

                status = "running" if state == "r" else "pending"
                node = queue_name if "@" in queue_name else "pending"

                # Get queue position and resource details
                queue_position = queue_positions.get(
                    job_id, 0) if status == "pending" else 0
                resource_requests = self._get_job_resources(job_id)

                jobs.append(GPUJob(
                    job_id=job_id,
                    task_name=task_name,
                    gpu_count=gpu_count,
                    cpu_count=cpu_count,
                    status=status,
                    node=node,
                    submit_time=submit_time,
                    priority=priority,
                    queue_position=queue_position,
                    resource_requests=resource_requests
                ))

            return jobs
        except (subprocess.CalledProcessError, ET.ParseError):
            return []

    def _get_queue_positions(self) -> Dict[str, int]:
        """Calculate queue positions for all pending jobs"""
        try:
            result = subprocess.run(
                ["qstat", "-xml"], capture_output=True, text=True, check=True)
            root = ET.fromstring(result.stdout)
        except (subprocess.CalledProcessError, ET.ParseError):
            return {}

        # Extract pending jobs with priorities
        pending_jobs = []
        for job_elem in root.findall(".//job_list[@state='pending']"):
            job_id = job_elem.findtext("JB_job_number")
            priority_text = job_elem.findtext("JAT_prio") or "0"
            try:
                priority = float(priority_text)
            except ValueError:
                priority = 0.0

            if job_id:
                pending_jobs.append((job_id, priority))

        # Sort by priority (higher priority = lower position in queue)
        pending_jobs.sort(key=lambda x: x[1], reverse=True)

        # Create position mapping
        positions = {}
        for i, (job_id, _) in enumerate(pending_jobs, 1):
            positions[job_id] = i

        return positions

    def _get_job_resources(self, job_id: str) -> Dict[str, str]:
        """Get detailed resource requests for a specific job"""
        try:
            result = subprocess.run(
                ["qstat", "-j", job_id], capture_output=True, text=True, check=True)
            output = result.stdout
        except subprocess.CalledProcessError:
            return {}

        resources = {}
        for line in output.split('\n'):
            if line.startswith('hard_resource_list:'):
                resource_part = line.split(':', 1)[1].strip()
                for item in resource_part.split(','):
                    if '=' in item:
                        key, value = item.split('=', 1)
                        resources[key.strip()] = value.strip()
                break

        return resources


class SystemMonitor:
    """ç³»ç»Ÿèµ„æºç›‘æ§å±‚"""

    def get_system_status(self) -> SystemStatus:
        """è·å–ç³»ç»Ÿèµ„æºçŠ¶æ€"""
        try:
            # CPU usage
            cpu_cmd = "grep 'cpu ' /proc/stat | awk '{usage=($2+$4)*100/($2+$3+$4+$5)} END {print usage}'"
            cpu_result = subprocess.run(
                cpu_cmd, shell=True, capture_output=True, text=True)
            cpu_percent = float(cpu_result.stdout.strip() or "0")

            # Memory usage
            mem_cmd = "free | grep Mem | awk '{printf \"%.1f\", $3/$2 * 100.0}'"
            mem_result = subprocess.run(
                mem_cmd, shell=True, capture_output=True, text=True)
            memory_percent = float(mem_result.stdout.strip() or "0")

            # Disk usage
            disk_cmd = "df / | tail -1 | awk '{print $5}' | cut -d'%' -f1"
            disk_result = subprocess.run(
                disk_cmd, shell=True, capture_output=True, text=True)
            disk_percent = float(disk_result.stdout.strip() or "0")

            # Load average
            load_cmd = "cat /proc/loadavg | awk '{print $1, $2, $3}'"
            load_result = subprocess.run(
                load_cmd, shell=True, capture_output=True, text=True)
            load_values = [float(x)
                           for x in load_result.stdout.strip().split()]

            # Uptime
            uptime_cmd = "cat /proc/uptime | awk '{print $1}'"
            uptime_result = subprocess.run(
                uptime_cmd, shell=True, capture_output=True, text=True)
            uptime = float(uptime_result.stdout.strip() or "0")

            # Process count
            proc_cmd = "ps aux | wc -l"
            proc_result = subprocess.run(
                proc_cmd, shell=True, capture_output=True, text=True)
            processes = int(proc_result.stdout.strip() or "0")

            return SystemStatus(
                cpu_percent=cpu_percent,
                memory_percent=memory_percent,
                disk_percent=disk_percent,
                load_average=load_values,
                uptime=uptime,
                processes=processes
            )

        except Exception:
            return SystemStatus(0, 0, 0, [0, 0, 0], 0, 0)


class GPUJobManager:
    """ä½œä¸šç®¡ç†å±‚ - è´Ÿè´£GPUç”³è¯·å’Œé‡Šæ”¾"""

    def __init__(self, config: Dict):
        self.config = config
        self.job_prefix = config.get("job_prefix", "NDFS_")

    def request_gpu(self, task_name: str) -> Optional[str]:
        """ç”³è¯·å•ä¸ªGPU"""
        job_name = f"{self.job_prefix}{task_name}_{int(time.time())}"

        cmd = [
            "qsub", "-q", "gpu",
            "-l", "gpu_card=1",
            "-N", job_name,
            "-b", "y",
            "sleep", "86400"  # 24 hours
        ]

        try:
            result = subprocess.run(
                cmd, capture_output=True, text=True, check=True
            )
            # Extract job ID
            match = re.search(r"Your job (\d+)", result.stdout)
            if match:
                job_id = match.group(1)
                print(f"âœ… ç”³è¯·æˆåŠŸ: Job {job_id} ({job_name})")
                return job_id
        except subprocess.CalledProcessError as e:
            print(f"âŒ ç”³è¯·å¤±è´¥: {e.stderr.strip()}")

        return None

    def release_gpu(self, job_id: str) -> bool:
        """é‡Šæ”¾GPU"""
        try:
            subprocess.run(["qdel", job_id], check=True)
            print(f"âœ… é‡Šæ”¾æˆåŠŸ: Job {job_id}")
            return True
        except subprocess.CalledProcessError as e:
            print(f"âŒ é‡Šæ”¾å¤±è´¥: {e.stderr.strip()}")
            return False

    def auto_manage(self, nodes: Dict[str, GPUNode], jobs: List[GPUJob]) -> None:
        """è‡ªåŠ¨ç®¡ç†GPUèµ„æº"""
        if not self.config.get("auto_manage", False):
            return

        # Count managed jobs
        managed_jobs = [
            job for job in jobs if job.task_name.startswith(self.job_prefix)]
        running_gpus = sum(
            job.gpu_count for job in managed_jobs if job.status == "running")
        pending_gpus = sum(
            job.gpu_count for job in managed_jobs if job.status == "pending")
        total_managed = running_gpus + pending_gpus

        min_required = self.config.get("min_reserved_gpus", 1)
        max_allowed = self.config.get("max_reserved_gpus", 2)
        max_total = self.config.get("max_total_gpus", 4)

        # Calculate free GPUs in cluster
        total_free = sum(node.free_gpus for node in nodes.values())

        # Auto acquire if below minimum
        if total_managed < min_required and total_managed < max_total and total_free > 0:
            needed = min(min_required - total_managed,
                         max_total - total_managed)
            print(
                f"ğŸ¤– è‡ªåŠ¨ç®¡ç†: å½“å‰{total_managed}ä¸ªGPUï¼Œéœ€è¦è‡³å°‘{min_required}ä¸ªï¼Œç”³è¯·{needed}ä¸ª")
            for i in range(needed):
                self.request_gpu(f"auto_{i+1}")


class ComprehensiveStatusDisplay:
    """ç»¼åˆçŠ¶æ€æ˜¾ç¤ºå±‚ - è´Ÿè´£å…¨é¢çš„çŠ¶æ€ä¿¡æ¯å±•ç¤º"""

    @staticmethod
    def show_status(nodes: Dict[str, GPUNode], jobs: List[GPUJob],
                    system_status: SystemStatus) -> None:
        """æ˜¾ç¤ºç»¼åˆçŠ¶æ€ä¿¡æ¯"""
        os.system('clear')

        print("\n" + "="*80)
        print(
            f"ğŸ–¥ï¸  ç»¼åˆGPUèµ„æºç®¡ç†å™¨ - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
        print(f"ğŸ‘¤ ç”¨æˆ·: {getpass.getuser()}")
        print("="*80)

        # é›†ç¾¤GPUçŠ¶æ€å’Œç³»ç»Ÿèµ„æºçŠ¶æ€æ¨ªå‘æ’åˆ—
        total_gpus = sum(node.total_gpus for node in nodes.values())
        free_gpus = sum(node.free_gpus for node in nodes.values())

        # è®¡ç®—æ€»CPUæ•°é‡
        total_running_cpus = sum(
            job.cpu_count for job in jobs if job.status == "running")
        total_pending_cpus = sum(
            job.cpu_count for job in jobs if job.status == "pending")

        print(f"\nğŸ“Š é›†ç¾¤çŠ¶æ€                        ğŸ’» ç³»ç»Ÿèµ„æº")
        print(
            f"   æ€»GPUæ•°: {total_gpus:<12}           CPUä½¿ç”¨ç‡: {system_status.cpu_percent:.1f}%")
        print(
            f"   ç©ºé—²GPU: {free_gpus:<12}           å†…å­˜ä½¿ç”¨ç‡: {system_status.memory_percent:.1f}%")
        print(
            f"   GPUä½¿ç”¨ç‡: {((total_gpus - free_gpus) / total_gpus * 100):.1f}%{'':6}     ç£ç›˜ä½¿ç”¨ç‡: {system_status.disk_percent:.1f}%")
        print(
            f"   æˆ‘çš„è¿è¡ŒCPU: {total_running_cpus:<8}        ç³»ç»Ÿè´Ÿè½½: {system_status.load_average[0]:.2f}")
        print(
            f"   æˆ‘çš„æ’é˜ŸCPU: {total_pending_cpus:<8}        è¿è¡Œæ—¶é—´: {system_status.format_uptime()}")
        print(f"{'':35}        è¿›ç¨‹æ•°: {system_status.processes}")

        # GPUèŠ‚ç‚¹è¯¦æƒ…
        if nodes:
            print(f"\nğŸ¢ GPUèŠ‚ç‚¹è¯¦æƒ…:")
            for node in sorted(nodes.values(), key=lambda x: x.name):
                status_icon = "ğŸŸ¢" if node.free_gpus > 0 else "ğŸ”´"
                usage_percent = ((node.total_gpus - node.free_gpus) /
                                 node.total_gpus * 100) if node.total_gpus > 0 else 0
                print(
                    f"   {status_icon} {node.name}: {node.free_gpus}/{node.total_gpus} ç©ºé—² ({usage_percent:.0f}%ä½¿ç”¨)")

        # æˆ‘çš„GPUä½œä¸šè¯¦æƒ…
        if jobs:
            running_jobs = [job for job in jobs if job.status == "running"]
            pending_jobs = [job for job in jobs if job.status == "pending"]

            print(f"\nğŸš€ æˆ‘çš„GPUä½œä¸šè¯¦æƒ…:")
            print(f"   è¿è¡Œä¸­: {len(running_jobs)} ä¸ªä½œä¸š")
            print(f"   æ’é˜Ÿä¸­: {len(pending_jobs)} ä¸ªä½œä¸š")

            if running_jobs:
                print(f"\n   ğŸ“‹ è¿è¡Œä¸­çš„ä½œä¸š:")
                for job in running_jobs:
                    resources_info = job.format_resources()
                    node_display = job.node.replace(
                        "gpu@", "") if "gpu@" in job.node else job.node
                    priority_display = f"{float(job.priority):.3f}" if job.priority and job.priority.replace(
                        '.', '').isdigit() else "N/A"
                    print(f"   ğŸ”µ Job {job.job_id}: {job.task_name}")
                    print(
                        f"        â””â”€ {resources_info} @ {node_display} (ä¼˜å…ˆçº§: {priority_display})")

            if pending_jobs:
                print(f"\n   â³ æ’é˜Ÿä¸­çš„ä½œä¸š:")
                for job in pending_jobs:
                    resources_info = job.format_resources()
                    position_info = f"é˜Ÿåˆ—ä½ç½®: #{job.queue_position}" if job.queue_position > 0 else "é˜Ÿåˆ—ä½ç½®: æœªçŸ¥"
                    priority_display = f"{float(job.priority):.3f}" if job.priority and job.priority.replace(
                        '.', '').isdigit() else "N/A"
                    print(f"   ğŸŸ¡ Job {job.job_id}: {job.task_name}")
                    print(
                        f"        â””â”€ {resources_info} ({position_info}, ä¼˜å…ˆçº§: {priority_display})")

            # ç»Ÿè®¡ä¿¡æ¯
            total_running_gpus = sum(job.gpu_count for job in running_jobs)
            total_pending_gpus = sum(job.gpu_count for job in pending_jobs)
            print(f"\n   ğŸ“ˆ ç»Ÿè®¡ä¿¡æ¯:")
            print(
                f"      è¿è¡Œä¸­GPU: {total_running_gpus} ä¸ª, CPU: {total_running_cpus} ä¸ª")
            print(
                f"      æ’é˜Ÿä¸­GPU: {total_pending_gpus} ä¸ª, CPU: {total_pending_cpus} ä¸ª")
            print(
                f"      æ€»è®¡GPU: {total_running_gpus + total_pending_gpus} ä¸ª, CPU: {total_running_cpus + total_pending_cpus} ä¸ª")
        else:
            print(f"\nğŸš€ æˆ‘çš„GPUä½œä¸š: æ— ")

        print("\n" + "="*80)
        print("ğŸ”§ ç®¡ç†åŠŸèƒ½: æŒ‰ Ctrl+C é€€å‡º")
        print("ğŸ’¡ æç¤º: ä½¿ç”¨ --help æŸ¥çœ‹æ›´å¤šé€‰é¡¹")
        print("="*80)


class ComprehensiveGPUManager:
    """ç»¼åˆGPUèµ„æºç®¡ç†å™¨ä¸»ç±»"""

    def __init__(self, config_file: str = "gpu_manager_config_simplified.json"):
        self.config_file = config_file
        self.config = self.load_config()
        self.gpu_monitor = GPUMonitor()
        self.system_monitor = SystemMonitor()
        self.job_manager = GPUJobManager(self.config)
        self.display = ComprehensiveStatusDisplay()
        self.running = False

        # æ³¨å†Œä¿¡å·å¤„ç†
        signal.signal(signal.SIGINT, self.shutdown)
        signal.signal(signal.SIGTERM, self.shutdown)

    def load_config(self) -> Dict:
        """åŠ è½½é…ç½®"""
        default_config = {
            "monitor_interval": 2,
            "min_reserved_gpus": 1,
            "max_reserved_gpus": 2,
            "max_total_gpus": 4,
            "job_prefix": "NDFS_",
            "auto_manage": True
        }

        if os.path.exists(self.config_file):
            try:
                with open(self.config_file, 'r') as f:
                    config = json.load(f)
                    default_config.update(config)
            except Exception:
                pass

        return default_config

    def shutdown(self, signum, frame):
        """ä¼˜é›…å…³é—­"""
        print("\næ­£åœ¨å…³é—­...")
        self.running = False
        sys.exit(0)

    def run_monitor(self):
        """è¿è¡Œç›‘æ§æ¨¡å¼"""
        print("ğŸ” å¯åŠ¨ç»¼åˆç›‘æ§æ¨¡å¼...")
        self.running = True

        try:
            while self.running:
                nodes = self.gpu_monitor.get_gpu_nodes()
                jobs = self.gpu_monitor.get_my_jobs()
                system_status = self.system_monitor.get_system_status()

                self.display.show_status(nodes, jobs, system_status)
                time.sleep(self.config["monitor_interval"])
        except KeyboardInterrupt:
            pass

    def run_manage(self):
        """è¿è¡Œç®¡ç†æ¨¡å¼"""
        print("ğŸ¤– å¯åŠ¨è‡ªåŠ¨ç®¡ç†æ¨¡å¼...")
        self.running = True

        try:
            while self.running:
                nodes = self.gpu_monitor.get_gpu_nodes()
                jobs = self.gpu_monitor.get_my_jobs()
                system_status = self.system_monitor.get_system_status()

                # è‡ªåŠ¨ç®¡ç†
                self.job_manager.auto_manage(nodes, jobs)

                # æ˜¾ç¤ºçŠ¶æ€
                self.display.show_status(nodes, jobs, system_status)
                time.sleep(self.config["monitor_interval"])
        except KeyboardInterrupt:
            pass

    def show_status(self):
        """æ˜¾ç¤ºå•æ¬¡çŠ¶æ€"""
        nodes = self.gpu_monitor.get_gpu_nodes()
        jobs = self.gpu_monitor.get_my_jobs()
        system_status = self.system_monitor.get_system_status()
        self.display.show_status(nodes, jobs, system_status)

    def request_gpus(self, count: int):
        """æ‰‹åŠ¨ç”³è¯·GPU"""
        print(f"ğŸ“ ç”³è¯· {count} ä¸ªGPU...")
        for i in range(count):
            job_id = self.job_manager.request_gpu(f"manual_{i+1}")
            if not job_id:
                break
            time.sleep(0.5)  # é¿å…è¿‡å¿«æäº¤

    def release_job(self, job_id: str):
        """é‡Šæ”¾ä½œä¸š"""
        self.job_manager.release_gpu(job_id)


def main():
    parser = argparse.ArgumentParser(description="ç»¼åˆGPUèµ„æºç®¡ç†å™¨")
    parser.add_argument("--config", default="gpu_manager_config_simplified.json",
                        help="é…ç½®æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--status", action="store_true",
                        help="æ˜¾ç¤ºçŠ¶æ€åé€€å‡º")
    parser.add_argument("--manage", action="store_true",
                        help="å¯åŠ¨è‡ªåŠ¨ç®¡ç†æ¨¡å¼")
    parser.add_argument("--request", type=int, metavar="N",
                        help="ç”³è¯·Nä¸ªGPU")
    parser.add_argument("--release", metavar="JOB_ID",
                        help="é‡Šæ”¾æŒ‡å®šä½œä¸š")

    args = parser.parse_args()

    manager = ComprehensiveGPUManager(args.config)

    if args.status:
        manager.show_status()
    elif args.manage:
        manager.run_manage()
    elif args.request:
        manager.request_gpus(args.request)
    elif args.release:
        manager.release_job(args.release)
    else:
        # é»˜è®¤ç›‘æ§æ¨¡å¼
        manager.run_monitor()


if __name__ == "__main__":
    main()
