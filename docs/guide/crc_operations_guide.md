# Notre Dame CRC å¿«é€Ÿå‘½ä»¤æ‰‹å†Œ
## ğŸ¯ GPU èµ„æºç®¡ç†
### æ£€æŸ¥ GPU å¯ç”¨æ€§

```bash
free_gpus.sh @crc_gpu                   # æŸ¥çœ‹GPUèŠ‚ç‚¹ç©ºé—²æƒ…å†µ
nvidia-smi                              # æŸ¥çœ‹å½“å‰èŠ‚ç‚¹GPUçŠ¶æ€

qrsh -q gpu -l gpu_card=1 -pe smp 8 # ç”³è¯·äº¤äº’å¼GPU

exit # ä½¿ç”¨å®Œæ¯•é‡Šæ”¾èµ„æº
```

### æ‰¹å¤„ç† GPU ä½œä¸š

```bash
# åˆ›å»ºä½œä¸šè„šæœ¬ gpu_job.sh
cat > gpu_job.sh << 'EOF'
#!/bin/bash
#$ -M zchen27@nd.edu
#$ -m abe
#$ -pe smp 4
#$ -q gpu
#$ -l gpu_card=1
#$ -N my_gpu_job

module load conda
conda activate myenv
python train.py
EOF

# æäº¤å’Œç®¡ç†ä½œä¸š
qsub gpu_job.sh                         # æäº¤ä½œä¸š
qstat -u zchen27                        # æŸ¥çœ‹æˆ‘çš„ä½œä¸š
qstat -j <job_id>                       # æŸ¥çœ‹ä½œä¸šè¯¦æƒ…
qdel <job_id>                           # åˆ é™¤ä½œä¸š
```
---

## ğŸ”§ å¸¸è§é—®é¢˜è§£å†³

```bash
# condaå‘½ä»¤æ‰¾ä¸åˆ°
module load conda

# æ£€æŸ¥ä½œä¸šä¸ºä»€ä¹ˆå¤±è´¥
qstat -j <job_id>

# æŸ¥çœ‹ä½œä¸šè¾“å‡º
cat <job_name>.o<job_id>
cat <job_name>.e<job_id>

# å¼ºåˆ¶åˆ é™¤å¡ä½çš„ä½œä¸š
qdel -f <job_id>
```

---

## ğŸ“‹ å¿«é€Ÿå¯åŠ¨æµç¨‹

```bash
# 1. ç™»å½•
ssh zchen27@crcfe01.crc.nd.edu

# 2. è®¾ç½®ç¯å¢ƒ
module load conda
conda activate myproject

# 3. æ£€æŸ¥GPUèµ„æº
gpu-free

# 4. ç”³è¯·GPUæµ‹è¯•
gpu-qrsh
nvidia-smi
exit

# 5. æäº¤æ­£å¼ä½œä¸š
qsub gpu_job.sh
myjobs
```

---

## ğŸ“ ä½œä¸šè„šæœ¬æ¨¡æ¿

### åŸºç¡€ GPU ä½œä¸š

```bash
#!/bin/bash
#$ -M your_email@nd.edu
#$ -m abe
#$ -pe smp 4
#$ -q gpu
#$ -l gpu_card=1
#$ -N job_name

module load conda
conda activate env_name
python your_script.py
```

### å¤š GPU ä½œä¸š

```bash
#!/bin/bash
#$ -M your_email@nd.edu
#$ -m abe
#$ -pe smp 8
#$ -q gpu
#$ -l gpu_card=2
#$ -N multi_gpu_job

module load conda
conda activate env_name
python -m torch.distributed.launch --nproc_per_node=2 train.py
```

---

_æœ€åæ›´æ–°ï¼š2025-07-16 | ç»´æŠ¤è€…ï¼šzchen27_
